version: "3.9"

services:

  # Vector store condiviso (memoria persistente)
  vectorstore:
    image: chromadb/chromadb:latest
    container_name: vectorstore
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - vectorstore-data:/data

  # Ollama LLM leggero / quantizzato
  ollama:
    image: ghcr.io/ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./ollama-data:/root/.ollama
      - ollama-memory:/root/.ollama/memory
    environment:
      - OLLAMA_LOG_LEVEL=info
      - VECTORSTORE_URL=http://vectorstore:8000
    command: >
      /bin/bash -c "
      ollama pull llama2-7b-4bit &&
      ollama serve --host 0.0.0.0 --port 11434
      "
    depends_on:
      - vectorstore

  # Qwen modello piccolo per task secondari
  qwen:
    image: some/qwen:3b-light   # sostituire con immagine 3B quantizzata disponibile
    container_name: qwen
    restart: unless-stopped
    ports:
      - "11435:11435"
    volumes:
      - ./qwen-data:/root/.qwen
    environment:
      - VECTORSTORE_URL=http://vectorstore:8000
    depends_on:
      - vectorstore

  # WebUI centralizzata
  webui:
    image: ghcr.io/openwebui/openwebui:latest
    container_name: webui
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - QWEN_HOST=http://qwen:11435
      - VECTORSTORE_URL=http://vectorstore:8000
    volumes:
      - ./webui-data:/app/data
    depends_on:
      - ollama
      - qwen
      - vectorstore

volumes:
  ollama-data:
  ollama-memory:
  qwen-data:
  webui-data:
  vectorstore-data:
